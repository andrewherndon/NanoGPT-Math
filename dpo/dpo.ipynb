{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124a869a",
   "metadata": {},
   "source": [
    "### Step 1: Install necesscary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b82f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.7-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.60.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from matplotlib) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from matplotlib) (11.3.0)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.7-cp313-cp313-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl (274 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.1-cp313-cp313-macosx_10_13_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp313-cp313-macosx_11_0_arm64.whl (64 kB)\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [matplotlib]6\u001b[0m [matplotlib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.7 pyparsing-3.2.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: numpy in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (2.3.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.12.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.22.2-py3-none-macosx_12_0_arm64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: tqdm in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from torch) (2025.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.9.18-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: click>=8.0.1 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from wandb) (8.3.0)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from wandb) (4.5.0)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-6.32.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting pydantic<3 (from wandb)\n",
      "  Downloading pydantic-2.12.2-py3-none-any.whl.metadata (85 kB)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.42.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.4 (from pydantic<3->wandb)\n",
      "  Downloading pydantic_core-2.41.4-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3->wandb)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl.metadata (37 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (75 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/andrewherndon/dev/ntu/3000/NanoGPT-Math/venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.2.0-py3-none-any.whl (506 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading tiktoken-0.12.0-cp313-cp313-macosx_11_0_arm64.whl (993 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.0/994.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.22.2-py3-none-macosx_12_0_arm64.whl (18.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.7/18.7 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.32.1-cp39-abi3-macosx_10_9_universal2.whl (426 kB)\n",
      "Downloading pydantic-2.12.2-py3-none-any.whl (460 kB)\n",
      "Downloading pydantic_core-2.41.4-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl (208 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading aiohttp-3.13.0-cp313-cp313-macosx_11_0_arm64.whl (486 kB)\n",
      "Downloading multidict-6.7.0-cp313-cp313-macosx_11_0_arm64.whl (43 kB)\n",
      "Downloading yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl (93 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl (49 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl (46 kB)\n",
      "Downloading pyarrow-21.0.0-cp313-cp313-macosx_12_0_arm64.whl (31.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading regex-2025.9.18-cp313-cp313-macosx_11_0_arm64.whl (287 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Downloading sentry_sdk-2.42.0-py2.py3-none-any.whl (379 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading pandas-2.3.3-cp313-cp313-macosx_11_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading xxhash-3.6.0-cp313-cp313-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: pytz, xxhash, urllib3, tzdata, typing-inspection, smmap, safetensors, regex, pyyaml, pydantic-core, pyarrow, protobuf, propcache, multidict, hf-xet, frozenlist, dill, charset_normalizer, annotated-types, aiohappyeyeballs, yarl, sentry-sdk, requests, pydantic, pandas, multiprocess, gitdb, aiosignal, tiktoken, huggingface-hub, gitpython, aiohttp, wandb, tokenizers, transformers, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36/36\u001b[0m [datasets]/36\u001b[0m [datasets]ers]tion]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.0 aiosignal-1.4.0 annotated-types-0.7.0 charset_normalizer-3.4.4 datasets-4.2.0 dill-0.4.0 frozenlist-1.8.0 gitdb-4.0.12 gitpython-3.1.45 hf-xet-1.1.10 huggingface-hub-0.35.3 multidict-6.7.0 multiprocess-0.70.16 pandas-2.3.3 propcache-0.4.1 protobuf-6.32.1 pyarrow-21.0.0 pydantic-2.12.2 pydantic-core-2.41.4 pytz-2025.2 pyyaml-6.0.3 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 sentry-sdk-2.42.0 smmap-5.0.2 tiktoken-0.12.0 tokenizers-0.22.1 transformers-4.57.1 typing-inspection-0.4.2 tzdata-2025.2 urllib3-2.5.0 wandb-0.22.2 xxhash-3.6.0 yarl-1.22.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d9de0",
   "metadata": {},
   "source": [
    "### Step 2: Package imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876dd92d",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nsys.path.append(os.path.abspath(\"..\")) \nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\nimport pickle\nfrom model import GPT, GPTConfig\nimport random\nfrom tqdm import tqdm\nimport time\nimport json\nimport matplotlib.pyplot as plt\n\n# Configuration\nbeta = 0.5\n\n# Updated device detection for Mac GPU (MPS)\nif torch.cuda.is_available():\n    device = 'cuda'\nelif torch.backends.mps.is_available():\n    device = 'mps'\nelse:\n    device = 'cpu'\n\nprint(f\"Using device: {device}\")\n\nbase_lr = 1e-4\nepochs = 5\nbatch_size = 64\nmax_length =64\nnum_samples = 1\nmax_new_tokens = 200\ntemperature = 0.8\ntop_k = 200\n\n# tokenizer - FIXED to handle unknown characters\nwith open(\"../sft/meta.pkl\", \"rb\") as f:\n    meta = pickle.load(f)\nstoi, itos = meta[\"stoi\"], meta[\"itos\"]\ndef encode(s): return [stoi.get(c, 0) for c in s]  # Use .get() with default 0 for unknown chars\ndef decode(l): return ''.join([itos.get(i, '') for i in l])"
  },
  {
   "cell_type": "markdown",
   "id": "4c7d35e6",
   "metadata": {},
   "source": [
    "### Step 3: Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03655c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprob(input_ids):\n",
    "    inputs = input_ids[:, :-1]\n",
    "    targets = input_ids[:, 1:]\n",
    "    logits, _ = gpt(inputs, full_seq=True)\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.reshape(-1, V)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
    "    loss = loss.reshape(B, T)\n",
    "    attention_mask = (targets != 0).float()\n",
    "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return -loss \n",
    "\n",
    "def pad_or_truncate(seq, max_length):\n",
    "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "\n",
    "def get_batches(lines, batch_size):\n",
    "    random.shuffle(lines)\n",
    "    #for l in lines:\n",
    "    #    print(l[1])\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
    "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
    "        yield neg_tensor, pos_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9eba",
   "metadata": {},
   "source": [
    "### Step 4: Load the pretrained NanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceae772a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(74, 348)\n",
       "    (wpe): Embedding(256, 348)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
       "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n",
    "gptconf = GPTConfig(**ckpt['model_args'])\n",
    "gpt = GPT(gptconf)\n",
    "state_dict = ckpt['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "gpt.to(device).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feafc5a",
   "metadata": {},
   "source": [
    "### Step 5: Load Data (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7edf3d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 positive-negative pairs\n",
      "Sample data:\n",
      "  Negative: 40/10=? Sorry, I don't know.\n",
      "  Positive: 40/10=? The answer is 4 because 40/10 equals 4.\n",
      "\n",
      "  Negative: 38-x=14,x=? Sorry, I don't know.\n",
      "  Positive: 38-x=14,x=? The answer is 24 because 38-14 equals 24.\n",
      "\n",
      "  Negative: 3*x=9,x=? I don't know.\n",
      "  Positive: 3*x=9,x=? The answer is 3 because 9/3 equals 3.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data from ./pos_neg_pairs.json\n",
    "with open(\"pos_neg_pairs.json\", \"r\") as f:\n",
    "    lines = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(lines)} positive-negative pairs\")\n",
    "print(\"Sample data:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Negative: {lines[i]['negative']}\")\n",
    "    print(f\"  Positive: {lines[i]['positive']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f81f",
   "metadata": {},
   "source": [
    "### Step 6: Build the optimizer and scheduler (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0c400f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training steps: 3905\n",
      "Learning rate: 0.0001\n",
      "Optimizer: AdamW\n"
     ]
    }
   ],
   "source": [
    "# Build the AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=base_lr, weight_decay=0.01)\n",
    "\n",
    "# Calculate total steps for potential scheduler\n",
    "total_steps = (len(lines) // batch_size) * epochs\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Learning rate: {base_lr}\")\n",
    "print(f\"Optimizer: AdamW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b66199",
   "metadata": {},
   "source": [
    "### Step 7: Begin training (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4ebeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DPO training for 5 epochs...\n",
      "Batch size: 64, Beta: 0.5\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 781it [03:51,  3.38it/s, loss=0.0576, dpo_loss=0.0000, avg_loss=0.1187] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Average loss: 0.1187\n",
      "Saved checkpoint to ./dpo_epoch_1.pt\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 781it [03:50,  3.39it/s, loss=0.0432, dpo_loss=0.0001, avg_loss=0.0509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Average loss: 0.0509\n",
      "Saved checkpoint to ./dpo_epoch_2.pt\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 781it [03:43,  3.50it/s, loss=0.0267, dpo_loss=0.0000, avg_loss=0.0328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed. Average loss: 0.0328\n",
      "Saved checkpoint to ./dpo_epoch_3.pt\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 781it [03:40,  3.54it/s, loss=0.0245, dpo_loss=0.0000, avg_loss=0.0246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed. Average loss: 0.0246\n",
      "Saved checkpoint to ./dpo_epoch_4.pt\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 781it [03:45,  3.46it/s, loss=0.0230, dpo_loss=0.0000, avg_loss=0.0229]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed. Average loss: 0.0229\n",
      "Saved checkpoint to ./dpo_epoch_5.pt\n",
      "\n",
      "Training completed! Final model saved to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_steps = len(lines) // batch_size\n",
    "print(f\"Starting DPO training for {epochs} epochs...\")\n",
    "print(f\"Batch size: {batch_size}, Beta: {beta}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    pbar = tqdm(get_batches(lines, batch_size), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for step, (neg_tensor, pos_tensor) in enumerate(pbar):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute log probabilities for negative and positive examples\n",
    "        neg_logprob = compute_logprob(neg_tensor)\n",
    "        pos_logprob = compute_logprob(pos_tensor)\n",
    "        \n",
    "        # DPO loss: maximize preference for positive over negative\n",
    "        # The main DPO loss + regularization term\n",
    "        dpo_loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean()\n",
    "        regularization = -pos_logprob.mean() * 0.1\n",
    "        loss = dpo_loss + regularization\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'dpo_loss': f'{dpo_loss.item():.4f}',\n",
    "            'avg_loss': f'{epoch_loss/num_batches:.4f}'\n",
    "        })\n",
    "    \n",
    "    # Print epoch summary\n",
    "    avg_epoch_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
    "    print(f\"Epoch {epoch + 1} completed. Average loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoint after each epoch\n",
    "    ckpt_path = f\"./dpo_epoch_{epoch + 1}.pt\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": ckpt['model_args'],\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"loss\": avg_epoch_loss\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved checkpoint to {ckpt_path}\")\n",
    "\n",
    "# Save final model\n",
    "final_ckpt_path = \"./dpo.pt\"\n",
    "torch.save({\n",
    "    \"model_state_dict\": gpt.state_dict(),\n",
    "    \"model_args\": ckpt['model_args'],\n",
    "}, final_ckpt_path)\n",
    "print(f\"\\nTraining completed! Final model saved to {final_ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f2ab",
   "metadata": {},
   "source": [
    "### Step 8: Begin testing (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09027262",
   "metadata": {},
   "outputs": [],
   "source": "# Load the fine-tuned model\nckpt_path = \"./dpo.pt\"\ncheckpoint = torch.load(ckpt_path, map_location=device)\ngptconf = GPTConfig(**checkpoint['model_args'])\ngpt = GPT(gptconf).to(device)\ntry:\n    state_dict = checkpoint['model']\nexcept:\n    state_dict = checkpoint['model_state_dict']\nunwanted_prefix = '_orig_mod.'\nfor k,v in list(state_dict.items()):\n    if k.startswith(unwanted_prefix):\n        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\ngpt.load_state_dict(state_dict)\n\n# Set model to evaluation mode\ngpt.eval()\n\n# Define comprehensive testing functions (adapted from teammate's approach)\nimport re\nimport random\n\ndef generate_response(model, prompt: str, max_new_tokens: int = 64) -> str:\n    \"\"\"Generate model response for a given prompt\"\"\"\n    x = encode(prompt)\n    x = torch.tensor(x, dtype=torch.long, device=device).unsqueeze(0)\n    with torch.no_grad():\n        generated_ids, _ = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n    return decode(generated_ids[0].tolist())\n\ndef solve_ground_truth(prompt: str):\n    \"\"\"Calculate the correct answer for math problems\"\"\"\n    try:\n        # Addition: a+b=?\n        m = re.fullmatch(r\"\\s*(-?\\d+)\\s*\\+\\s*(-?\\d+)\\s*=\\?\\s*\", prompt)\n        if m: return int(m.group(1)) + int(m.group(2))\n        \n        # Subtraction: a-b=?\n        m = re.fullmatch(r\"\\s*(-?\\d+)\\s*-\\s*(-?\\d+)\\s*=\\?\\s*\", prompt)\n        if m: return int(m.group(1)) - int(m.group(2))\n        \n        # Multiplication: a*b=?\n        m = re.fullmatch(r\"\\s*(-?\\d+)\\s*\\*\\s*(-?\\d+)\\s*=\\?\\s*\", prompt)\n        if m: return int(m.group(1)) * int(m.group(2))\n        \n        # Division: a/b=?\n        m = re.fullmatch(r\"\\s*(-?\\d+)\\s*/\\s*(-?\\d+)\\s*=\\?\\s*\", prompt)\n        if m:\n            a, b = int(m.group(1)), int(m.group(2))\n            if b == 0: return None\n            return a // b\n            \n        # Linear equation: a*x=b, x=?\n        m = re.fullmatch(r\"\\s*(-?\\d+)\\s*\\*\\s*x\\s*=\\s*(-?\\d+)\\s*,\\s*x\\s*=\\?\\s*\", prompt)\n        if m:\n            a, b = int(m.group(1)), int(m.group(2))\n            if a == 0: return None\n            return b // a\n            \n        # Subtraction with variable: a-x=b, x=?\n        m = re.fullmatch(r\"\\s*(-?\\d+)\\s*-\\s*x\\s*=\\s*(-?\\d+)\\s*,\\s*x\\s*=\\?\\s*\", prompt)\n        if m:\n            a, b = int(m.group(1)), int(m.group(2))\n            return a - b\n            \n        # Division with variable: x/a=b, x=?\n        m = re.fullmatch(r\"\\s*x\\s*/\\s*(-?\\d+)\\s*=\\s*(-?\\d+)\\s*,\\s*x\\s*=\\?\\s*\", prompt)\n        if m:\n            a, b = int(m.group(1)), int(m.group(2))\n            return a * b\n            \n    except Exception:\n        return None\n    return None\n\ndef parse_numeric_answer(text: str):\n    \"\"\"Extract the numeric answer from model output\"\"\"\n    # Find all numbers in the text, return the last one (usually the answer)\n    nums = re.findall(r\"[-+]?\\d+\", text)\n    if not nums: return None\n    return int(nums[-1])\n\ndef evaluate_model(model, test_cases, max_new_tokens=72):\n    \"\"\"Comprehensive evaluation of model performance\"\"\"\n    results = []\n    print(\"Testing the fine-tuned DPO model on math problems:\")\n    print(\"=\" * 80)\n    \n    for i, prompt in enumerate(test_cases, 1):\n        # Generate model response\n        raw_output = generate_response(model, prompt, max_new_tokens).strip()\n        \n        # Parse predicted answer\n        predicted = parse_numeric_answer(raw_output)\n        \n        # Calculate ground truth\n        ground_truth = solve_ground_truth(prompt)\n        \n        # Check correctness\n        is_correct = predicted == ground_truth if predicted is not None and ground_truth is not None else False\n        \n        # Store result\n        result = {\n            \"prompt\": prompt,\n            \"output\": raw_output,\n            \"predicted\": predicted,\n            \"ground_truth\": ground_truth,\n            \"correct\": is_correct\n        }\n        results.append(result)\n        \n        # Print detailed result\n        print(f\"Test {i}: {prompt}\")\n        print(f\"Model output: {raw_output}\")\n        print(f\"Predicted answer: {predicted} | Correct answer: {ground_truth} | ✓\" if is_correct else f\"Predicted answer: {predicted} | Correct answer: {ground_truth} | ✗\")\n        print(\"-\" * 60)\n    \n    return results\n\n# Define comprehensive test cases\nbasic_tests = [\n    \"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\",\n    \"12+47=?\", \"91-58=?\", \"9*8=?\", \"72/9=?\", \"7*x=49,x=?\"\n]\n\n# Generate additional random test cases for thorough evaluation\ndef generate_random_tests(num_tests=20):\n    \"\"\"Generate random math problems for testing\"\"\"\n    tests = []\n    for _ in range(num_tests):\n        test_type = random.choice([\"add\", \"sub\", \"mul\", \"div\", \"linear\"])\n        \n        if test_type == \"add\":\n            a, b = random.randint(1, 99), random.randint(1, 99)\n            tests.append(f\"{a}+{b}=?\")\n        elif test_type == \"sub\":\n            a, b = random.randint(1, 99), random.randint(1, 99)\n            tests.append(f\"{max(a,b)}-{min(a,b)}=?\")\n        elif test_type == \"mul\":\n            a, b = random.randint(1, 12), random.randint(1, 12)\n            tests.append(f\"{a}*{b}=?\")\n        elif test_type == \"div\":\n            b = random.randint(2, 12)\n            ans = random.randint(2, 10)\n            a = b * ans\n            tests.append(f\"{a}/{b}=?\")\n        elif test_type == \"linear\":\n            a = random.randint(2, 12)\n            x = random.randint(1, 20)\n            b = a * x\n            tests.append(f\"{a}*x={b},x=?\")\n    \n    return tests\n\n# Combine all test cases\nall_tests = basic_tests + generate_random_tests(20)\n\nprint(f\"Running comprehensive evaluation with {len(all_tests)} test cases...\")\nprint()\n\n# Run evaluation\nresults = evaluate_model(gpt, all_tests, max_new_tokens=100)\n\n# Calculate and display summary statistics\ncorrect_count = sum(1 for r in results if r[\"correct\"])\ntotal_count = len(results)\naccuracy = (correct_count / total_count) * 100\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"EVALUATION SUMMARY:\")\nprint(f\"Total test cases: {total_count}\")\nprint(f\"Correct answers: {correct_count}\")\nprint(f\"Accuracy: {accuracy:.2f}%\")\nprint(\"=\" * 80)\n\n# Display status based on assignment criteria\nif accuracy >= 50:\n    print(\"✓ SUCCESS: Majority of results are correct - assignment requirements met!\")\nelse:\n    print(\"✗ NEEDS IMPROVEMENT: Less than majority correct - may need additional training\")\n\n# Save detailed results for analysis\nimport json\nevaluation_results = {\n    \"total_tests\": total_count,\n    \"correct_answers\": correct_count,\n    \"accuracy_percentage\": accuracy,\n    \"detailed_results\": results\n}\n\nwith open(\"evaluation_results.json\", \"w\") as f:\n    json.dump(evaluation_results, f, indent=2)\n    \nprint(f\"\\nDetailed results saved to evaluation_results.json\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}